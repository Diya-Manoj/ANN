{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4771de5a",
   "metadata": {},
   "source": [
    "## Assignment 6 - A43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afefd02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1febc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # initialise weights\n",
    "        self.w1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.w2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "    \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    \n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1-x)\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        #Calulate output of hidden layer\n",
    "        self.z = np.dot(X, self.w1)\n",
    "        self.z2 = self.sigmoid(self.z)\n",
    "        \n",
    "        # calculate output of output Layer\n",
    "        self.z3 = np.dot(self.z2, self.w2) \n",
    "        output = self.sigmoid(self.z3)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def backward(self, x, y, output):\n",
    "        #calculate the error and derivative of error for output Layer\n",
    "        self.output_error = y - output\n",
    "        self.output_delta = self.output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        #calculate error and derivative of error for hidden Layer\n",
    "        self.z2_error = self.output_delta.dot(self.w2.T)\n",
    "        self.z2_delta = self.z2_error * self.sigmoid_derivative(self.z2)\n",
    "        \n",
    "        #update weights\n",
    "        self.w1 += X.T.dot(self.z2_delta)\n",
    "        self.w2 += self.z2.T.dot(self.output_delta)\n",
    "    \n",
    "    \n",
    "    def train(self, x, y, epochs):\n",
    "        for i in range(epochs):\n",
    "            #forward propagation\n",
    "            output = self.forward(X)\n",
    "            \n",
    "            #backward propagation\n",
    "            self.backward(x, y, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0321a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a neural network object by specifying the number of inputs, hidden units, \n",
    "nn = NeuralNetwork(input_size=2,hidden_size=3, output_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a74203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify train data 'X' and target output 'y'\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y= np.array([[0], [1], [1], [0]])\n",
    "\n",
    "#train the network\n",
    "nn.train(X, y, epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a3797de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99235145]\n",
      " [0.98228111]\n",
      " [0.87144413]\n",
      " [0.35061391]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions on new data\n",
    "new_data = np.array([[0, 0.5], [0, 0.8], [1, 0.2], [1, 0.6]])\n",
    "predictions = nn.forward(new_data)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3a20b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'losses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Plotting loss\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(nn\u001b[38;5;241m.\u001b[39mlosses)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'losses'"
     ]
    }
   ],
   "source": [
    "# Base neural network (continued from your experiment)\n",
    "'''Hereâ€™s the extended version of your code with the following modifications:\n",
    "\n",
    "ReLU activation support (with switchable activation).\n",
    "\n",
    "Bias terms added to both layers.\n",
    "\n",
    "Learning rate as a parameter.\n",
    "\n",
    "Loss visualization using Matplotlib.'''\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size, activation=\"sigmoid\", learning_rate=1.0):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.w1 = np.random.randn(self.input_size, self.hidden_size)\n",
    "        self.b1 = np.random.randn(self.hidden_size)\n",
    "        self.w2 = np.random.randn(self.hidden_size, self.output_size)\n",
    "        self.b2 = np.random.randn(self.output_size)\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = self.sigmoid\n",
    "            self.activation_derivative = self.sigmoid_derivative\n",
    "        elif activation == \"relu\":\n",
    "            self.activation = self.relu\n",
    "            self.activation_derivative = self.relu_derivative\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        self.z = np.dot(X, self.w1) + self.b1\n",
    "        self.z2 = self.activation(self.z)\n",
    "        self.z3 = np.dot(self.z2, self.w2) + self.b2\n",
    "        self.output = self.sigmoid(self.z3)  # Always sigmoid at output\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y):\n",
    "        output_error = y - self.output\n",
    "        output_delta = output_error * self.sigmoid_derivative(self.output)\n",
    "\n",
    "        z2_error = output_delta.dot(self.w2.T)\n",
    "        z2_delta = z2_error * self.activation_derivative(self.z2)\n",
    "\n",
    "        self.w2 += self.learning_rate * self.z2.T.dot(output_delta)\n",
    "        self.b2 += self.learning_rate * np.sum(output_delta, axis=0)\n",
    "        self.w1 += self.learning_rate * self.input.T.dot(z2_delta)\n",
    "        self.b1 += self.learning_rate * np.sum(z2_delta, axis=0)\n",
    "\n",
    "        return np.mean(np.abs(output_error))\n",
    "\n",
    "    def train(self, X, y, epochs):\n",
    "        self.losses = []\n",
    "        for _ in range(epochs):\n",
    "            self.forward(X)\n",
    "            loss = self.backward(y)\n",
    "            self.losses.append(loss)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X)\n",
    "\n",
    "# Example usage:\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(input_size=2, hidden_size=3, output_size=1, activation=\"relu\", learning_rate=0.5)\n",
    "nn.train(X, y, epochs=10000)\n",
    "\n",
    "# Predict\n",
    "new_data = np.array([[0, 0.5], [0, 0.8], [1, 0.2], [1, 0.6]])\n",
    "predictions = nn.predict(new_data)\n",
    "print(\"Predictions:\\n\", predictions)\n",
    "\n",
    "# Plotting loss\n",
    "plt.plot(nn.losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias manually\n",
    "\n",
    "Add this in __init__:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "self.b1 = np.random.randn(self.hidden_size)\n",
    "self.b2 = np.random.randn(self.output_size)\n",
    "\n",
    "Update forward:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "self.z = np.dot(X, self.w1) + self.b1\n",
    "...\n",
    "self.z3 = np.dot(self.z2, self.w2) + self.b2\n",
    "\n",
    "\n",
    "Update backward:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "self.b1 += np.sum(self.z2_delta, axis=0)\n",
    "self.b2 += np.sum(self.output_delta, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
